{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPddtOk755piHjZmKSCvAQN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhuvaneswaran-k/Twitter-Scraping-Project-1/blob/main/Twitter_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmSWE4U0JheQ"
      },
      "outputs": [],
      "source": [
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import pymongo\n",
        "import datetime\n",
        "from datetime import date\n",
        "import time\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"Twitter Scraper\",\n",
        "    page_icon=\"chart_with_upwards_trend\",\n",
        "layout=\"wide\")\n",
        "\n",
        "st.header(\"\"\"\n",
        "Twitter Scrapping using Snscrape and Streamlit:\n",
        "\"\"\")\n",
        "\n",
        "today=date.today()\n",
        "NAME = st.text_input('What do you want to search for?')\n",
        "count = st.number_input('Enter the limit')\n",
        "start_date=st.date_input('enter the start date',datetime.date(2022, 1, 1))\n",
        "end_date=st.date_input('enter the end date',today)\n",
        "\n",
        "csv = st.radio(\"Download option\", ['CSV', 'Json', 'Export'])\n",
        "\n",
        "count = int(count)\n",
        "submit_button = st.button(label='Search')\n",
        "\n",
        "tweets_list1 = []\n",
        "\n",
        "if submit_button:\n",
        "    for i , tweet in enumerate(sntwitter.TwitterSearchScraper(f'{NAME} since:{start_date} until:{end_date}').get_items()):\n",
        "        if i > count-1:\n",
        "            break\n",
        "        tweets_list1.append(\n",
        "            [tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.likeCount, tweet.retweetCount,\n",
        "             tweet.sourceLabel, tweet.user.location])\n",
        "        tweet_df = pd.DataFrame(tweets_list1, columns=[\"Date\", \"Id\", \"Content\", \"Username\", \"LikeCount\", \"RetweetCount\",\n",
        "                                                       \"SourceLabel\", \"Location\"],index=None)\n",
        "\n",
        "    my_bar = st.progress(0)\n",
        "    for percent_complete in range(100):\n",
        "        time.sleep(0.05)\n",
        "        my_bar.progress(percent_complete + 1)\n",
        "    st.dataframe(tweet_df,)\n",
        "\n",
        "\n",
        "    st.success(\"you have Extracted the data \")\n",
        "\n",
        "    if csv == \"CSV\":\n",
        "        file_converted = tweet_df.to_csv()\n",
        "        st.download_button(\n",
        "            label=\"Download data as CSV\",\n",
        "            data=file_converted,\n",
        "            file_name='twits.csv',\n",
        "            mime='text/csv',\n",
        "        )\n",
        "    elif (csv == 'Json'):\n",
        "        file_converted = tweet_df.to_json()\n",
        "        st.download_button(\n",
        "            label=\"Download data as json\",\n",
        "            data=file_converted,\n",
        "            file_name='twits.json',\n",
        "            mime=\"application/json\",\n",
        "        )\n",
        "    elif csv == \"Export\":\n",
        "        export_button = st.button(label=\"Export\")\n",
        "        now = datetime.datetime.now()\n",
        "        client = pymongo.MongoClient(\"mongodb+srv://bhuvaneswarank:Krishna12@cluster0.7hwuocf.mongodb.net/?retryWrites=true&w=majority\")\n",
        "        db = client.Tweeter_scrap\n",
        "        records = db.Scrap_data\n",
        "        for i, tweet in enumerate(\n",
        "                sntwitter.TwitterSearchScraper(f'{NAME} since:{start_date} until:{end_date}').get_items()):\n",
        "            if i > count - 1:\n",
        "                break\n",
        "            tweets_list1.append(\n",
        "                [tweet.date, tweet.id, tweet.content, tweet.user.username, tweet.likeCount, tweet.retweetCount,\n",
        "                 tweet.sourceLabel, tweet.user.location])\n",
        "            tweet_df = pd.DataFrame(tweets_list1,\n",
        "                                    columns=[\"Date\", \"Id\", \"Content\", \"Username\", \"LikeCount\", \"RetweetCount\",\n",
        "                                             \"SourceLabel\", \"Location\"], index=None)\n",
        "            l = {\"Scraped_Name\": NAME, \"Time\": now, \"Scraped_data\": [\n",
        "                    {\"Date_Time\": tweet.date, \"Tweet_ID\": tweet.id, \"Tweet_content\": tweet.content,\n",
        "                     \"Username\": tweet.user.username,\n",
        "                     \"Like Count\": tweet.likeCount, \"ReTweet Count\": tweet.retweetCount, \"Source\": tweet.sourceLabel,\n",
        "                     \"Location\": tweet.user.location}]}\n",
        "            records.insert_one(l)\n",
        "        st.success(\"data has uploaded in mongodb\")"
      ]
    }
  ]
}